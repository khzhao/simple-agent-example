## 2048 LoRA PPO Skeleton
- Environment (`src/simple_agent_example/env/game.py`) mirrors the OpenPipe ART 2048 example: deterministic board updates, XML move parsing, and reward shaping toward the 128 tile.
- Rollout loop (`src/simple_agent_example/rollout.py`) streams text prompts to the policy, records `<move>â€¦</move>` completions, and bundles per-step metadata for PPO.
- `TinkerTrainableModel` (`src/simple_agent_example/tinker_client.py`) speaks directly to the official `tinker` SDK: it manages the ServiceClient, sampling client, tokenizer, LoRA forward/backward calls, optimizer steps, and checkpointing.
- Training harness (`src/simple_agent_example/train.py`) gathers concurrent on-policy trajectories, normalizes advantages, proxies them to the Tinker LoRA API, refreshes sampling weights, and repeats.

Install dependencies with `uv sync` (requires `python-dotenv`, `numpy`, `tinker`, `torch`). Set `TINKER_API_KEY` plus optional flags like `--rank`, `--temperature`, or `--enable-ruler`, then run `uv run train-2048` (or `python -m simple_agent_example.train`). Plug in your ruler/grading callback or additional logging as needed. Use `--wandb-project your_project` (and optionally `--wandb-run-name ...`) to stream rollout statistics to Weights & Biases; add `--no-wandb` to disable logging. If you want to pretrain format compliance, run `uv run sft-2048 --dataset data/sft/2048_sft_dataset.jsonl --save-name 2048-sft-init` to produce a warm-start LoRA checkpoint.
